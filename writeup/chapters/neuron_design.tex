The synthetic nervous system controller is developed with a non-spiking Hodgkin-Huxley neuron model, where the membrane voltage is used as a representation of spiking frequency of an interconnected population of spiking neurons. The neurons are modeled with a resting potential of -60 mV and a maximum potential of -40 mV for a range of 20 mV.

The change in voltage over time for the Hodgkin-Huxley model is determined as follows:

\begin{equation}
C_{m} \dfrac{d V}{d t} = I_{leak} + I_{syn} + I_{app}
\end{equation}

% TODO(buckbaskin): What is C_{m}?
The applied current ($I_{app}$) is arbitrary, and used with multiple neurons in the controller design.
The current terms are defined as follows:

\begin{equation}
I_{leak} = G_{m} (E_{r} - V)
\end{equation}

% TODO(buckbaskin): define terms in I_{leak} equation
The leak current ($I_{leak}$) moves the voltage towards its resting potential over time.

\begin{equation}
I_{syn} = \sum_{i=1}^{n} G_{s, i} (E_{s, i} - V)
\end{equation}

% TODO(buckbaskin): define terms in I_{syn} equation
The synaptic current ($I_{syn}$) is the current conducted from incoming synapses from other neurons based on their activation.

This model does leave out some terms of the Hodgkin-Huxley model. By continuing to follow the work laid out in \cite{NickFunctionalSubnetwork}, the steady state values of a neuron can be arranged in the following equation:

\begin{equation}
U_{post}^{*} = \dfrac{\frac{g_{s}}{R} U_{pre} \Delta E_{s}}{1 + \frac{g_{s}}{R} U_{pre}}
\end{equation}

$U_{post}^{*}$ is the steady state voltage of the output neuron, relative to its resting potential. $U_{pre}$ is the voltage of input neuron relative to its resting potential. $g_{s}$ is the conductivity of the synapse.
% TODO(buckbaskin): define terms in the U_{post} equation

Through the tuning of the conductivity of the synapse ($g_{s}$), one can engineer the behavior of each type of synapse (see \myref{sec:key_synapses}) to match arithmetic operations. The more complicated operations require the interaction of multiple synapses and neurons, but their behavior is still tuned in the same way.

\bbs{Key Neurons and Synapses}
\label{sec:key_synapses}

The neuron controller network is made up of a set of engineered synapses
designed to emulate arithmetic operations. These synapses can be grouped into two
categories: excitatory neurons and inhibitory neurons. 

Most neurons were tuned
via an optimization process to identify the best equilibrium potential and
synaptic conductance based on equations defined in 
\cite{NickFunctionalSubnetwork}. The above equation for determining $U_{post}$ was optimized to match a target dataset that exactly replicated the mathematical operation using a least squares approach.
Some neurons were tuned by hand for specific behaviors in
a subsection of the overall controller.

\bbss{Excitatory Synapses}

Except where otherwise noted, a value of 134 mV was used for the equilibrium
potential of excitatory synapses. The pre-synaptic threshold is -60 mV and the pre-synaptic saturation
level is -40 mV.

\bbsss{Signal Transfer}

The signal transfer synapse is designed to pass the voltage of the input neuron
to the output neuron in the active range of the neuron. The signal transfer synapse is often used to add
the value of two or more neurons together in an output neuron.
The signal transfer synapse has a synaptic 
conductance of 0.115 microsiemens. 

\bbsss{Inverted Signal Transfer}

By combining two signal inversions, a more accurate signal transfer synapse was
created. This involves an extra neuron; however, adding the neuron leads to a more precise
transfer of the voltage level of the input neuron to the output neuron. This tradeoff was used primarily between larger subnetworks. It also helps define boundaries for testing the larger subnetworks. The combined signal transfer synapse was implemented with two Signal Inverter (Stimulated)
synapses.

\bbsss{Signal Amplifier 2x}

The signal amplifier synapse is designed to pass the voltage of the input neuron
to the output neuron with a 2x gain. This synapse was tuned with input values from 0 to
10 mV. Higher inputs saturate the output neuron. It has a synaptic conductance
of 0.23 microsiemens.

\bbsss{Signal Amplifier 4x}

The 4x signal amplifier has the same function as the 2x amplifier but with a larger 
gain. This synapse was tuned with input values from 0 to
5 mV. Higher inputs saturate the output neuron. It has a synaptic conductance
of 0.46 microsiemens.

\bbsss{Signal Reduction 0.2x}

The signal reduction synapse is designed to pass the voltage of the input neuron
to the output neuron in the active range of the output neuron, but with a gain of 0.2x. 
It has a synaptic  conductance of 0.021 microsiemens. The signal reduction synapse was tuned
over the complete range of input values, 0 to 20 mV.

\bbsss{Signal Reduction 0.5x}

The signal reduction synapse has the same function as the 0.2x signal reducer, but with a loss of 0.5x. This signal reduction synapse has a synaptic  conductance of 0.054 microsiemens.

\bbsss{Convert Forward Positive}

The convert forward positive synapse is designed to convert the representation
of a value in a single neuron, for example the neuron that represents the 
position of the joint, to the same value represented in two neurons. This
synapse converts the value when the value is above -50 mV to a positive value between 0 
and 20 mV. The pre-synaptic
threshold of this synapse is -50 mV and its synaptic conductance is 0.115 microsiemens.

\bbsss{Torque Pressure Converter}

The torque pressure converter synapse is designed to perform the torque-pressure
calculation approximation. It has a synaptic 
conductance of 0.048 microsiemens.

\bbss{Inhibitory Synapses}

Except where otherwise noted, the equilibrium potential of inhibitory neurons
is simulated as -100 mV. This is the lowest value possible in the simulation.
The pre-synaptic threshold is -60 mV. The pre-synaptic saturation level is -40
mV.

\bbsss{Signal Inverter}

The signal inverted synapse is designed to decrease the voltage of the output 
neuron proportional to the voltage of the input neuron. It has a synaptic 
conductance of 0.55 microsiemens.

\bbsss{Signal Inverter (Stimulated)}

The signal inverted synapse is designed to decrease the voltage of the output 
neuron proportional to the voltage of the input neuron. This signal inverter (stimulated) synapse is tuned
slightly differently from the standard signal inverter due to an
observed difference in voltage when a stimulus current was applied to the output
neuron along with other incoming synapses. It has a synaptic 
conductance of 0.5 microsiemens.

\bbsss{Signal Inverter Reduction 0.2x}

The signal inverter reduction synapse is designed to decrease the voltage of
the output 
neuron at a 0.2x loss compared to the voltage of the input neuron. This synapse has a 
synaptic conductance of 0.093 microsiemens.

\bbsss{Signal Inverter Reduction 0.5x}

The signal inverter reduction synapse is designed to decrease the voltage of
the output 
neuron at a 0.5x loss compared to the voltage of the input neuron. This synapse has a 
synaptic conductance of 0.22 microsiemens.

\bbsss{Signal Inverter Amplifier 2x}

The signal inverter amplifier synapse is designed to decrease the voltage of
the output 
neuron at a 2x gain to the increasing voltage of the input neuron. It has a synaptic 
conductance of 1.11 microsiemens.

\bbsss{Signal Inverter Amplifier 4x}

The signal inverter amplifier synapse is designed to decrease the voltage of
the output 
neuron at a 4x gain to the increasing voltage of the input neuron. It has a synaptic 
conductance of 2.3 microsiemens.

\bbsss{Integral Inhibitor}

As originally described in \cite{NickFunctionalSubnetwork}, the integral inhibitor synapses are used to mutually inhibit a pair of neurons to hold both 
neurons at a marginally stable value. Without this mutual inhibition, a single neuron acts
as a leaky integrator and its voltage will decay to its resting potential over time without additional input.  The values for this
synapse are based on \cite{NickFunctionalSubnetwork}. The synapse has a  
conductance of 0.5 microsiemens.

\bbsss{Convert Forward Negative}

The convert forward negative synapse is designed to convert the representation
of a value in a single neuron, for example the neuron that represents the 
position of the joint, to the same value represented in two neurons. This
synapse converts the value when the value is below -50 mV to a positive value 
between 0 and 20 mV. The pre-synaptic
saturation of this synapse is -50 mV and its synaptic conductance is 0.5 microsiemens.

\bbsss{Signal Divider}

The signal divider synapse is designed to reduce the effect of another input
synapse on a mutual output neuron, where the reduction increases with increased input voltage to the
signal divider synapse. It is distinguished from the signal multiplier synapse
in that the value never reaches 0. Intuitively, the behavior for large inputs is a replication of 
division where dividing by a large number makes the quantity small but never 0.
The signal divider synapse has a synaptic conductance of 20 microsiemens. The equilibrium potential of
this synapse is -60 mV (equal to the resting potential of the input and output
neurons).

\bbsss{Signal Multiplier}

The signal multiplier synapse is designed to reduce the effect of another input
synapse, where the reduction decreases with decreased input voltage to the
signal multiplier synapse.  This is achieved by combining two signal division synapses in series.

The signal multiplier group is distinguished from the signal divider synapse
behavior in that the value reaches 0. Intuitively, the implementation is a replication of 
multiplication where multiplying by 0 will make any value 0.
The signal multiplier synapses have a synaptic conductance of 19.75 microsiemens. The equilibrium potential 
of these synapses is -61 mV.

\bbs{Sensor Fusion}

The sensor fusion neuron network performs essentially the same function as the
sensor fusion network in the prototype controller. In the synthetic neuron implementation, 3 neurons
represent the 3 sensor inputs available in a joint: position (``Theta"),
extension muscle pressure (``Ext Pres") and flexion muscle pressure
(``Flx Pres"). The outputs for the network are the estimates for current 
position, velocity and acceleration.

\bbss{Velocity Fusion Network Components}

\bbsss{Differentiator Network}

The velocity network is based on the differentiator network presented in 
\cite{NickFunctionalSubnetwork}. The key insight into making a network
of neurons that can estimate the derivative of an input signal is that
the time constants of individual neurons can be independently tuned to 
approximate a single step finite difference method. This insight is based
on a Reichardt detector network as explained in
\cite{NickFunctionalSubnetwork}.

\bbss{Velocity Fusion Network}

The velocity network is based on the differentiator network. 
The one major change from the differentiaotr network,
as presented, is the inclusion of a second output ($U_{post}$) neuron to 
represent the negative derivative of the position (negative velocity). See \myref{fig:SensorFusion}.

\begin{figure}
\centering
\includegraphics[width=5in]{methods/SensorFusion_Ext}
\caption{Sensor fusion network for acceleration (bottom left) and velocity (top right)}
\label{fig:SensorFusion}
\end{figure}

The use of a second neuron represents
a common pattern used across the network where two neurons are used to represent
a single value. One neuron represents positive levels of the variable and is 
at or below resting potential when the value is negative. The other neuron is
above resting potential for negative values and is at or below resting potential
when the value is positive. 

The motivation for increasing the complexity of the
network (often doubling the number of neurons and more than doubling the number of synapses) is to increase the
effective range of values that the neuron can represent at the same fidelity and
to increase the accuracy of zero. When a single neuron represents positive and
negative values of equal magnitude, the value of 0 is represented at 50 mV; 
however, after passing through a signal transfer synapse this value is often
slightly higher, up to 52 mV. This means that comparing the two neurons (the
original neuron and the signal transfer) yields a slightly positive error
instead of near zero error.

\bbss{Acceleration Fusion Network Components}

\bbsss{Absolute Value Network}

Within the acceleration fusion network, the absolute value of the position is
used. This is calculated by first splitting the position into its two neuron
representation. From there, the sum of the two neurons is used as the absolute
value. This approach takes advantage of the definition of each side of the two neuron
representation falling below resting potential when the other neuron is active.
This approach means the signal transfer synapse from the below zero neuron will have no
effect.

\bbsss{Integration Network}

The integration network used throughout the neuron controller is based heavily
on the Integrator Network in \cite{NickFunctionalSubnetwork}. Two neurons are
designed to mutually inhibit each other so that the combined pair hold their
values. Individual neurons can be treated as a leaky integrator; however, their
voltage tails off over time if there is no maintenance current. The integration
network itself is tuned by changing the time constant of the component neurons
to adjust how much the voltage of the integrator network changes for an input
current.

\bbsss{Convert Torque to Pressure}

One of the key observations for converting from desired torque to pressure 
within the neuron controller was a pattern observed during simulation. For the
simulated joint, the maximum torque can be applied at an angle of 0 degrees. See \myref{fig:PositionTorque}.

\begin{figure}
\centering
\includegraphics[width=5in]{neuron_design/Pos_v_Torque2.png}
\caption{Position/torque relation for pneumatic muscle actuated joint. See \github{stability/constant\_pressure.py}}
\label{fig:PositionTorque}
\end{figure}

With increasing deflection, there is decreasing torque applied for a given
pressure, or phrased another way, a greater pressure differential is required
for the same desired torque (\myref{fig:PressureTorque} and \github{stability/pressure\_torque.py}). When the calculation is rearranged to calculate
a control pressure from a desired torque at a given position, the relationship
between the two is linear for that position for most desired torques above
0.25 Nm in the simulated joint (see \myref{fig:PressureTorque}. Assuming there is some non-zero antagonistic
torque, each actuator should fall into the near-linear range. This assumption allows for
an approximation of the linear section in the neuron model.

\begin{figure}
\centering
\includegraphics[width=5in]{neuron_design/FigPressureTorque}
\caption{Torque/Pressure relation observed in simulation}
\label{fig:PressureTorque}
\end{figure}

The neurons in the controller approximate the torque/pressure relationship using a multiplication synapse added
to a signal transfer synapse. The multiplication takes the scaled absolute 
value of the position as input, so that as the position of the joint reaches
the joint limit, the slope of the torque to pressure curve increases. This
approximates the observations in simulation with a relatively simple neuron
network. See \myref{fig:T2PNetwork} for the assembled network.

\begin{figure}
\centering
\includegraphics[width=5in]{methods/TestVisualT2P}
\caption{Torque to pressure conversion network}
\label{fig:T2PNetwork}
\end{figure}

\bbsss{Pressure Estimation Loop}

Within the acceleration network, there is a feedback loop that is used to 
estimate the torque applied by a given pressure. First, the loop ``initializes"
with an extension torque guess from the integrator. Second, the extension
torque is converted into extension pressure. Third, the estimated extension
pressure is compared with the sensed pressure. If there is any difference between the
two, the extension torque guess is modified in turn and the cycle repeats.

This architecture is mirrored for flexion torque.

\bbsss{Torque to Acceleration Network}

The torque to acceleration network is built up in 3 stages. First, a damping 
term is subtracted or added based on the current velocity and the estimated
damping factor. Second, the conservative load is applied to the joint. Third,
the acceleration is attenuated proportional to the mass with a signal divider
synapse.

The scaling of the damping factor is the most complicated component of this
network. First, the estimated damping factor is scaled based on the velocity 
with a multiplication synapse to ensure that damping has no effect on acceleration
when the velocity is zero. Second, the estimated damping factor is then applied with both positive and
negative velocities (in a split representation) to the positive and negative
acceleration terms. Due to the design choice of representing the velocity as two
neurons, one term will be 0 and the other will have a 
non-zero effect. In code, the comparable calculation would be done as an if statement. In neurons,
all the values and effects are calculated in practice, but only some are passed
through the network.

\bbss{Acceleration Fusion Network}

The acceleration fusion network is a combination of an integrator, absolute
value network, the pressure estimation loop and a torque to acceleration
network. In total, the network uses a combination of smaller networks to
estimate the torque applied from sensed pressure and then combine the extension
and flexion torques together to get a net torque and acceleration. For the complete network, see \myref{fig:SensorFusion}.

\bbs{Optimizing Torque Control}

The torque control network uses a feedback loop to optimize the
control torque. The network takes the estimated position and velocity from the 
sensor fusion network and the desired position from a trajectory generator
network (ex. a CPG) and outputs the torque to best follow that trajectory.
See \myref{fig:TorqueOptimizationNetwork} for the complete network.

\begin{figure}
\centering
\includegraphics[width=5in]{methods/TorqueOptimization}
\caption{Feedback loop for determining the desired control torque}
\label{fig:TorqueOptimizationNetwork}
\end{figure}

\bbss{Optimizing Torque Control Components}

\bbsss{Torque Estimation}

The first step in the torque control network is a torque estimate. This is built
as an integration network.

\bbsss{Torque to Acceleration}

This torque to acceleration network is calculated in the same manner as above.
The principle difference is that the source of torque is a desired control
torque instead of the estimated applied torque of the actuators.

\bbsss{Velocity Modification}

The velocity modification network takes the current velocity as an input and adjusts for
the acceleration. Based on the estimated update rate of the controller, the
change in velocity is linear in acceleration.

\begin{equation}
\dot{\theta}_{future} = \dot{\theta}_{current} + \ddot{\theta} \delta t
\end{equation}

This is encoded as a constant gain (calculated to be 0.2x) to be incorporated 
into to the velocity voltages.

\bbsss{Position Modification}

The position modification network takes the modified velocity as an input and adjusts the
future position for a time step in the same method as the velocity modification
network.

\begin{equation}
\theta_{future} = \theta_{current} + \dot{\theta}_{future} \delta t
\end{equation}


\bbsss{Position Comparator}

The position comparator estimates the error between where the joint will be at
the future point and time and where the trajectory generator wants the joint
to be. This error is used as feedback to directly modify the torque estimate.
This network geometry allows for easy incorporation of other metrics as well.
For example, if the position estimation is noisy, multiple forward projections
can be made for upper and lower bounds and the torque can be optimized to 
minimize an error function over all projected locations.

\bbss{Torque Control Network}

The overall feedback loop and torque control network is a combination of pieces
that join together to approximate
Newton's Method for finding the root of the error function.

The original 
controller used a bounded bisection method that proved less suited for 
implementation in neurons. The bisection method was originally chosen because
it would always converge or have a well defined behavior when the desired
position fell outside the bounds.

\begin{equation}
\theta_{future} = \theta_{current} + (\dot{\theta}_{current} + \ddot{\theta} \delta t) \delta t
\end{equation}

\begin{equation}
\theta_{future} = \theta_{current} + \dot{\theta}_{current} \delta t + \ddot{\theta} \delta t^{2}
\end{equation}

\begin{equation}
\dfrac{\partial \theta_{future}}{\partial \ddot{\theta}} = \delta t^{2}
\end{equation}

The derivative of the position is constant with respect to the controlled value
$\ddot{\theta}$. Therefore, Newton's Method should also always converge and it is a suitable replacement for the bisection method in the synthetic neuron
controller. In practice, Newton's Method should work well with many error functions; however,
a more complicated error function involving multiple independent error 
components may exhibit local minima and a more complex root finding feedback
method may be required.

\bbs{Torque to Pressure}

The torque to pressure section of the network is smaller than the rest of the 
major components. It is built in the same way as the torque to pressure inside
the sensor fusion pressure to torque network. In the sensor fusion application,
the network is used in a forward capacity inside of a feedback loop that ends
up performing an inverse. The torque to pressure network is much simpler because this network
is used only in a forward capacity to compute the desired pressures.

One additional modification to this network is the inclusion of a stiffness
parameter neuron. This neuron controls any desired overlap in the antagonistic control
to increase or decrease joint stiffness manually.

\bbs{System Modeling}

The system modeling network compares the predicted location of the joint from 
the last control iteration with the current position. Using the error 
calculations discussed in \myref{chap:controller_design}, the neuron network
can calculate the weight updates for its internal estimate of damping and 
conservative load. The equations are as follows:

\begin{equation}
\lambda 
=
- \dfrac{2M}{\delta t^{2}} \dfrac{\theta_{err}}{1 + \dot{\theta}_{0}^{2}}
\end{equation}

\begin{equation}
C_{err} = \lambda \dot{\theta}, N_{err} = \lambda
\end{equation}

\begin{figure}
\centering
\includegraphics[width=6.5in]{methods/SystemModel_Test}
\caption{System modeling network}
\label{fig:SystemModelNetwork}
\end{figure}

\myref{fig:SystemModelNetwork} shows the complete network.

\bbss{System Modeling Components}

\bbsss{Theta Delay}

The most unique component of the system is the delayed neuron that approximates
the estimated position of the joint. The delayed neuron is implemented with a signal transfer
synapse and a single neuron with a time constant that is tuned to the controller
update rate.

\bbsss{Prediction Error}

The prediction error is calculated in a dual neuron representation. The
error is then multiplied with the inertia, again resulting in a dual 
representation of the positive and negative quantity. This matches the behavior
expressed in the first term of the update equation to calculate $\lambda$ (equation \ref{eq:lambda_def}).

\bbsss{Velocity Correction}

The absolute value of the velocity is used along with a signal divider. This
matches the second term of the equation to calculate $\lambda$ from equation \ref{eq:lambda_def}. In practice,
the divider network is one of the networks that fits closely to a term like
$\dfrac{1}{1 + x^{2}}$. One potential improvement to the network is to fit
a custom synapse or subnetwork to better fit the term; however, the velocity estimate is not
currently a major source of error.

\bbsss{Lambda Calculation}

The calculation of $\lambda$ is completed with the inclusion of a gain term. This extra neuron and synapse
allows for the scaling of the updates and behaves in a similar way to the 
learning rate in a neural network. A single data point may be noisy, but if a 
series of updates suggest the same parameter update, the network will adjust the
parameter.

\bbsss{Calculating $\Delta N$}

The calculation for the conservative load is simply a signal transfer from the scaled lambda, following equation \ref{eq:find_C}.

\bbsss{Calculating $\Delta C$}

The calculation for the damping factor update is more involved. In particular,
the complexity comes from the fact that the sign of the update changes with the
sign of the velocity and the positional error. Intuitively, the change in sign occurs when
damping opposes velocity, but conservative loads (as modelled) do not depend on
velocity. For example, a positive velocity and positive $\lambda$ means that 
less actual damping occurred than was expected, but a negative velocity with a 
positive $\lambda$ means that more damping occurred than was expected.

To perform the damping factor calculation in neurons, the velocity and $\lambda$ are 
calculated for all sign pairs. From there, the correct outputs are connected.
Following the example above, a positive velocity and positive $\lambda$ transfer
their signal to the positive C Delta output, which then connects to the damping factor 
to drive the stored value down. For a negative velocity and positive $\lambda$, the ``C Err -+" neuron activates and transfers activation to the negative C Delta output.

This implementation relies on the dual neuron representation for values to drive
the correct output. Either the positive or negative neuron will be active in the
velocity and $\lambda$ values, therefore only one of the 4 combination neurons
will be active meaning that only the correct sign of the output value will be
active, maintaining the correctness of the dual neuron representation in the 
output. This implementation replaces a series of if statements that may appear
in a code-defined controller and shows an additional advantage of using two
neurons to represent a value in a functional subnetwork. When connected 
correctly, the dual neuron representation of single values allows for easier sign flipping, easier validation of correctness
by checking edge connectivity, and easier testing.

\bbs{Combined Controller Network}

The fully combined network implements
a controller by combining the 4 large subnetworks. First, sensor data is passed
through a sensor fusion network. The state estimation output of the sensor 
fusion network is then passed into the torque optimization network. The 
selected torque is then passed through one more subnetwork to convert the
desired torque into pressures for the actuators.

This leaves the system model subnetwork. This last subnetwork is connected to 
the state estimation values and the internal projection and observes the 
differences between the forward prediction and reality. This network then 
updates key model parameters based on its observations to improve the controller over time while 
maintaining a controller that avoids excessive oscillation. 
